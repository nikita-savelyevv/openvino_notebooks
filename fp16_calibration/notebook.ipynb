{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:41:11.719336600Z",
     "start_time": "2023-11-20T14:41:00.030512700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 23:48:44.611636: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-20 23:48:44.612732: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-20 23:48:44.633862: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-20 23:48:44.634317: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-20 23:48:45.058125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
      "Compiling the model to GPU ...\n"
     ]
    }
   ],
   "source": [
    "from utils import SUPPORTED_MODELS\n",
    "from transformers import AutoConfig\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"red-pajama-3b-chat\"\n",
    "# models_dir = Path(\"c:/Users/nsavelye/workspace/projects/openvino_notebooks/notebooks/254-llm-chatbot\")\n",
    "models_dir = Path(\"./\")\n",
    "model_dir = models_dir / model_id / \"FP16\"\n",
    "# model_dir = models_dir / model_id / \"FP16_calibrated\"\n",
    "# model_dir = models_dir / model_id / \"INT8_compressed_weights\"\n",
    "model_configuration = SUPPORTED_MODELS[model_id]\n",
    "\n",
    "core = ov.Core()\n",
    "# device = \"CPU\"\n",
    "device = \"GPU\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(models_dir / model_id / \"RedPajama-INCITE-Chat-3B-v1\", trust_remote_code=True)\n",
    "\n",
    "ov_config = {'PERFORMANCE_HINT': 'LATENCY', 'NUM_STREAMS': '1', \"CACHE_DIR\": \"\"}\n",
    "ov_model = OVModelForCausalLM.from_pretrained(model_dir, device=device, ov_config=ov_config,\n",
    "                                              config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "                                              trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████████████████████▎                                                                                          | 1/4 [00:50<02:31, 50.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upcasted node __module.model.gpt_neox.layers.1.mlp.dense_4h_to_h/aten::linear/MatMul_702 with 0.10 rel2_diff_ratio 0.049910 and mean_rel_error 0.035067\n",
      "Upcasted node __module.model.gpt_neox.layers.2.mlp.dense_4h_to_h/aten::linear/MatMul_937 with 0.10 rel2_diff_ratio 0.047837 and mean_rel_error 0.034047\n",
      "Upcasted node __module.model.gpt_neox.layers.3.mlp.dense_4h_to_h/aten::linear/MatMul_1172 with 0.10 rel2_diff_ratio 0.048047 and mean_rel_error 0.034022\n",
      "Upcasted node __module.model.gpt_neox.layers.4.mlp.dense_4h_to_h/aten::linear/MatMul_1407 with 0.10 rel2_diff_ratio 0.047386 and mean_rel_error 0.033766\n",
      "Upcasted node __module.model.gpt_neox.layers.5.mlp.dense_4h_to_h/aten::linear/MatMul_1642 with 0.10 rel2_diff_ratio 0.048077 and mean_rel_error 0.033411\n",
      "Upcasted node __module.model.gpt_neox.layers.6.mlp.dense_4h_to_h/aten::linear/MatMul_1877 with 0.10 rel2_diff_ratio 0.045523 and mean_rel_error 0.033319\n",
      "Upcasted node __module.model.gpt_neox.layers.7.mlp.dense_4h_to_h/aten::linear/MatMul_2112 with 0.10 rel2_diff_ratio 0.047206 and mean_rel_error 0.033780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████████▌                                                            | 2/4 [01:40<01:40, 50.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upcasted node __module.model.gpt_neox.layers.8.mlp.dense_4h_to_h/aten::linear/MatMul_2347 with 0.10 rel2_diff_ratio 0.049399 and mean_rel_error 0.034660\n",
      "Upcasted node __module.model.gpt_neox.layers.9.mlp.dense_4h_to_h/aten::linear/MatMul_2582 with 0.10 rel2_diff_ratio 0.048197 and mean_rel_error 0.033691\n",
      "Upcasted node __module.model.gpt_neox.layers.10.mlp.dense_4h_to_h/aten::linear/MatMul_2817 with 0.10 rel2_diff_ratio 0.046725 and mean_rel_error 0.033831\n",
      "Upcasted node __module.model.gpt_neox.layers.11.mlp.dense_4h_to_h/aten::linear/MatMul_3052 with 0.10 rel2_diff_ratio 0.044802 and mean_rel_error 0.033132\n",
      "Upcasted node __module.model.gpt_neox.layers.12.mlp.dense_4h_to_h/aten::linear/MatMul_3287 with 0.10 rel2_diff_ratio 0.045763 and mean_rel_error 0.032555\n",
      "Upcasted node __module.model.gpt_neox.layers.13.mlp.dense_4h_to_h/aten::linear/MatMul_3522 with 0.10 rel2_diff_ratio 0.049339 and mean_rel_error 0.035232\n",
      "Upcasted node __module.model.gpt_neox.layers.14.mlp.dense_4h_to_h/aten::linear/MatMul_3757 with 0.10 rel2_diff_ratio 0.049700 and mean_rel_error 0.035051\n",
      "Upcasted node __module.model.gpt_neox.layers.15.mlp.dense_4h_to_h/aten::linear/MatMul_3992 with 0.10 rel2_diff_ratio 0.051052 and mean_rel_error 0.035919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████▊                              | 3/4 [02:30<00:50, 50.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upcasted node __module.model.gpt_neox.layers.16.mlp.dense_4h_to_h/aten::linear/MatMul_4227 with 0.10 rel2_diff_ratio 0.048858 and mean_rel_error 0.034349\n",
      "Upcasted node __module.model.gpt_neox.layers.17.mlp.dense_4h_to_h/aten::linear/MatMul_4462 with 0.10 rel2_diff_ratio 0.049639 and mean_rel_error 0.034044\n",
      "Upcasted node __module.model.gpt_neox.layers.18.mlp.dense_4h_to_h/aten::linear/MatMul_4697 with 0.10 rel2_diff_ratio 0.048738 and mean_rel_error 0.035155\n",
      "Upcasted node __module.model.gpt_neox.layers.19.mlp.dense_4h_to_h/aten::linear/MatMul_4932 with 0.10 rel2_diff_ratio 0.050240 and mean_rel_error 0.035955\n",
      "Upcasted node __module.model.gpt_neox.layers.20.mlp.dense_4h_to_h/aten::linear/MatMul_5167 with 0.10 rel2_diff_ratio 0.047837 and mean_rel_error 0.035032\n",
      "Upcasted node __module.model.gpt_neox.layers.21.mlp.dense_4h_to_h/aten::linear/MatMul_5402 with 0.10 rel2_diff_ratio 0.048678 and mean_rel_error 0.034784\n",
      "Upcasted node __module.model.gpt_neox.layers.22.mlp.dense_4h_to_h/aten::linear/MatMul_5637 with 0.10 rel2_diff_ratio 0.052464 and mean_rel_error 0.037058\n",
      "Upcasted node __module.model.gpt_neox.layers.23.mlp.dense_4h_to_h/aten::linear/MatMul_5872 with 0.10 rel2_diff_ratio 0.053726 and mean_rel_error 0.037875\n",
      "Upcasted node __module.model.gpt_neox.layers.24.mlp.dense_4h_to_h/aten::linear/MatMul_6107 with 0.10 rel2_diff_ratio 0.052163 and mean_rel_error 0.036863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [03:14<00:00, 48.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upcasted node __module.model.gpt_neox.layers.25.mlp.dense_4h_to_h/aten::linear/MatMul_6342 with 0.10 rel2_diff_ratio 0.050962 and mean_rel_error 0.035390\n",
      "Upcasted node __module.model.gpt_neox.layers.26.mlp.dense_4h_to_h/aten::linear/MatMul_6577 with 0.10 rel2_diff_ratio 0.055138 and mean_rel_error 0.037367\n",
      "Upcasted node __module.model.gpt_neox.layers.27.mlp.dense_4h_to_h/aten::linear/MatMul_6812 with 0.10 rel2_diff_ratio 0.050962 and mean_rel_error 0.036008\n",
      "Upcasted node __module.model.gpt_neox.layers.28.mlp.dense_4h_to_h/aten::linear/MatMul_7047 with 0.10 rel2_diff_ratio 0.049219 and mean_rel_error 0.035956\n",
      "Upcasted node __module.model.gpt_neox.layers.29.mlp.dense_4h_to_h/aten::linear/MatMul_7282 with 0.10 rel2_diff_ratio 0.040986 and mean_rel_error 0.030744\n",
      "Upcasted node __module.model.gpt_neox.layers.30.mlp.dense_4h_to_h/aten::linear/MatMul_7517 with 0.10 rel2_diff_ratio 0.041647 and mean_rel_error 0.031767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling the model to GPU ...\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import shutil\n",
    "import partially_upcast_nodes_to_fp32\n",
    "import model_upcast_utils\n",
    "import main\n",
    "importlib.reload(partially_upcast_nodes_to_fp32)\n",
    "importlib.reload(main)\n",
    "\n",
    "\n",
    "model = partially_upcast_nodes_to_fp32.partially_upcast_nodes_to_fp32(ov_model.model, main.get_inputs_for_calibration(\n",
    "    ov_model, tok, model_configuration, \"<human>: Which lakes are near Munich?\\n<bot>:\"), batch_size=50, verbose=True)\n",
    "# model = model_upcast_utils.partially_upcast_nodes_to_fp32(ov_model.model, get_inputs_for_calibration(\"<human>: Which lakes are near Munich?\\n<bot>:\"))\n",
    "calibrated_model_dir = Path(f\"{model_dir}_calibrated\")\n",
    "ov.save_model(model, calibrated_model_dir / \"openvino_model.xml\")\n",
    "shutil.copy(model_dir / \"config.json\", calibrated_model_dir / \"config.json\")\n",
    "# ov_model = OVModelForCausalLM.from_pretrained(calibrated_model_dir, device=device, ov_config=ov_config,\n",
    "#                                               config=AutoConfig.from_pretrained(calibrated_model_dir, trust_remote_code=True),\n",
    "#                                               trust_remote_code=True)\n",
    "ov_model.model = model\n",
    "ov_model._original_model = model\n",
    "ov_model.request = None\n",
    "ov_model.compile()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:44:36.859984700Z",
     "start_time": "2023-11-20T14:41:11.719336600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/guest/nsavelye/venvs/fp16_calibration/lib/python3.8/site-packages/optimum/intel/openvino/modeling_decoder.py:388: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which lakes are near Munich?\n",
      "<bot>: There is a total of five large and beautiful alpine lakes in the Bavarian Alps.  The largest lake, Lake Starnberg (Starnberger See) has an area of about 100 km2 with over 200 islands that can be reached by boat or on foot from two small towns: Obertrum/Trumersee and Unterach am Attersee which have both been popular destinations for tourists since Roman times due to their warm water springs. The second-largest"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import main\n",
    "importlib.reload(main)\n",
    "\n",
    "prompt = \"Which lakes are near Munich?\"\n",
    "\n",
    "generation_kwargs = dict(\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.1,\n",
    "        do_sample=0.1 > 0.0,\n",
    "        top_p=1.0,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "\n",
    "# print(run_generate(ov_model, prompt, model_configuration, **generation_kwargs))\n",
    "for text in main.run_generate(ov_model, tok, prompt, model_configuration, **generation_kwargs):\n",
    "    print(text, end=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:44:48.861691700Z",
     "start_time": "2023-11-20T14:44:36.859984700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T14:44:48.866704800Z",
     "start_time": "2023-11-20T14:44:48.863688200Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
