{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T14:38:36.095867100Z",
     "start_time": "2023-10-31T14:38:22.024487600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/nsavelye/venvs/fp16_calibration/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
      "Compiling the model to GPU ...\n"
     ]
    }
   ],
   "source": [
    "from utils import SUPPORTED_MODELS\n",
    "from transformers import AutoConfig\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"red-pajama-3b-chat\"\n",
    "# models_dir = Path(\"c:/Users/nsavelye/workspace/projects/openvino_notebooks/notebooks/254-llm-chatbot\")\n",
    "models_dir = Path(\"./\")\n",
    "model_dir = models_dir / model_id / \"FP16\"\n",
    "# model_dir = models_dir / model_id / \"FP16_calibrated\"\n",
    "# model_dir = models_dir / model_id / \"INT8_compressed_weights\"\n",
    "model_configuration = SUPPORTED_MODELS[model_id]\n",
    "\n",
    "core = ov.Core()\n",
    "# device = \"CPU\"\n",
    "device = \"GPU\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(models_dir / model_id / \"RedPajama-INCITE-Chat-3B-v1\", trust_remote_code=True)\n",
    "\n",
    "ov_config = {'PERFORMANCE_HINT': 'LATENCY', 'NUM_STREAMS': '1', \"CACHE_DIR\": \"\"}\n",
    "ov_model = OVModelForCausalLM.from_pretrained(model_dir, device=device, ov_config=ov_config,\n",
    "                                              config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "                                              trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 25%|██▌       | 1/4 [00:53<02:40, 53.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upcasted node __module.model.gpt_neox.layers.1.mlp.dense_4h_to_h/aten::linear/MatMul_543 with 0.10 rel2_diff_ratio 0.050180 and mean_rel_error 0.035856\n",
      "Upcasted node __module.model.gpt_neox.layers.2.mlp.dense_4h_to_h/aten::linear/MatMul_766 with 0.10 rel2_diff_ratio 0.046124 and mean_rel_error 0.033481\n",
      "Upcasted node __module.model.gpt_neox.layers.3.mlp.dense_4h_to_h/aten::linear/MatMul_989 with 0.10 rel2_diff_ratio 0.047626 and mean_rel_error 0.033770\n",
      "Upcasted node __module.model.gpt_neox.layers.4.mlp.dense_4h_to_h/aten::linear/MatMul_1212 with 0.10 rel2_diff_ratio 0.045343 and mean_rel_error 0.033719\n",
      "Upcasted node __module.model.gpt_neox.layers.5.mlp.dense_4h_to_h/aten::linear/MatMul_1435 with 0.10 rel2_diff_ratio 0.046034 and mean_rel_error 0.033424\n",
      "Upcasted node __module.model.gpt_neox.layers.6.mlp.dense_4h_to_h/aten::linear/MatMul_1658 with 0.10 rel2_diff_ratio 0.046875 and mean_rel_error 0.033980\n",
      "Upcasted node __module.model.gpt_neox.layers.7.mlp.dense_4h_to_h/aten::linear/MatMul_1881 with 0.10 rel2_diff_ratio 0.045883 and mean_rel_error 0.033657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [01:45<01:45, 52.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upcasted node __module.model.gpt_neox.layers.8.mlp.dense_4h_to_h/aten::linear/MatMul_2104 with 0.10 rel2_diff_ratio 0.048618 and mean_rel_error 0.034572\n",
      "Upcasted node __module.model.gpt_neox.layers.9.mlp.dense_4h_to_h/aten::linear/MatMul_2327 with 0.10 rel2_diff_ratio 0.046965 and mean_rel_error 0.034050\n",
      "Upcasted node __module.model.gpt_neox.layers.10.mlp.dense_4h_to_h/aten::linear/MatMul_2550 with 0.10 rel2_diff_ratio 0.047236 and mean_rel_error 0.034274\n",
      "Upcasted node __module.model.gpt_neox.layers.11.mlp.dense_4h_to_h/aten::linear/MatMul_2773 with 0.10 rel2_diff_ratio 0.045913 and mean_rel_error 0.033175\n",
      "Upcasted node __module.model.gpt_neox.layers.12.mlp.dense_4h_to_h/aten::linear/MatMul_2996 with 0.10 rel2_diff_ratio 0.046635 and mean_rel_error 0.033646\n",
      "Upcasted node __module.model.gpt_neox.layers.13.mlp.dense_4h_to_h/aten::linear/MatMul_3219 with 0.10 rel2_diff_ratio 0.047476 and mean_rel_error 0.035168\n",
      "Upcasted node __module.model.gpt_neox.layers.14.mlp.dense_4h_to_h/aten::linear/MatMul_3442 with 0.10 rel2_diff_ratio 0.048588 and mean_rel_error 0.034375\n",
      "Upcasted node __module.model.gpt_neox.layers.15.mlp.dense_4h_to_h/aten::linear/MatMul_3665 with 0.10 rel2_diff_ratio 0.050391 and mean_rel_error 0.035291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [02:38<00:52, 52.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upcasted node __module.model.gpt_neox.layers.16.mlp.dense_4h_to_h/aten::linear/MatMul_3888 with 0.10 rel2_diff_ratio 0.046965 and mean_rel_error 0.033864\n",
      "Upcasted node __module.model.gpt_neox.layers.17.mlp.dense_4h_to_h/aten::linear/MatMul_4111 with 0.10 rel2_diff_ratio 0.047296 and mean_rel_error 0.034661\n",
      "Upcasted node __module.model.gpt_neox.layers.18.mlp.dense_4h_to_h/aten::linear/MatMul_4334 with 0.10 rel2_diff_ratio 0.047716 and mean_rel_error 0.034095\n",
      "Upcasted node __module.model.gpt_neox.layers.19.mlp.dense_4h_to_h/aten::linear/MatMul_4557 with 0.10 rel2_diff_ratio 0.050901 and mean_rel_error 0.036590\n",
      "Upcasted node __module.model.gpt_neox.layers.20.mlp.dense_4h_to_h/aten::linear/MatMul_4780 with 0.10 rel2_diff_ratio 0.047626 and mean_rel_error 0.034472\n",
      "Upcasted node __module.model.gpt_neox.layers.21.mlp.dense_4h_to_h/aten::linear/MatMul_5003 with 0.10 rel2_diff_ratio 0.047897 and mean_rel_error 0.035550\n",
      "Upcasted node __module.model.gpt_neox.layers.22.mlp.dense_4h_to_h/aten::linear/MatMul_5226 with 0.10 rel2_diff_ratio 0.053516 and mean_rel_error 0.037874\n",
      "Upcasted node __module.model.gpt_neox.layers.23.mlp.dense_4h_to_h/aten::linear/MatMul_5449 with 0.10 rel2_diff_ratio 0.052284 and mean_rel_error 0.037264\n",
      "Upcasted node __module.model.gpt_neox.layers.24.mlp.dense_4h_to_h/aten::linear/MatMul_5672 with 0.10 rel2_diff_ratio 0.051232 and mean_rel_error 0.035996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [03:24<00:00, 51.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upcasted node __module.model.gpt_neox.layers.25.mlp.dense_4h_to_h/aten::linear/MatMul_5895 with 0.10 rel2_diff_ratio 0.051983 and mean_rel_error 0.036955\n",
      "Upcasted node __module.model.gpt_neox.layers.26.mlp.dense_4h_to_h/aten::linear/MatMul_6118 with 0.10 rel2_diff_ratio 0.054778 and mean_rel_error 0.038841\n",
      "Upcasted node __module.model.gpt_neox.layers.27.mlp.dense_4h_to_h/aten::linear/MatMul_6341 with 0.10 rel2_diff_ratio 0.052734 and mean_rel_error 0.035784\n",
      "Upcasted node __module.model.gpt_neox.layers.28.mlp.dense_4h_to_h/aten::linear/MatMul_6564 with 0.10 rel2_diff_ratio 0.051713 and mean_rel_error 0.036293\n",
      "Upcasted node __module.model.gpt_neox.layers.29.mlp.dense_4h_to_h/aten::linear/MatMul_6787 with 0.10 rel2_diff_ratio 0.040535 and mean_rel_error 0.030602\n",
      "Upcasted node __module.model.gpt_neox.layers.30.mlp.dense_4h_to_h/aten::linear/MatMul_7010 with 0.10 rel2_diff_ratio 0.040685 and mean_rel_error 0.031799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling the model to GPU ...\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import shutil\n",
    "import partially_upcast_nodes_to_fp32\n",
    "import model_upcast_utils\n",
    "import main\n",
    "importlib.reload(partially_upcast_nodes_to_fp32)\n",
    "importlib.reload(main)\n",
    "\n",
    "\n",
    "model = partially_upcast_nodes_to_fp32.partially_upcast_nodes_to_fp32(ov_model.model, main.get_inputs_for_calibration(\n",
    "    ov_model, tok, model_configuration, \"<human>: Which lakes are near Munich?\\n<bot>:\"), batch_size=50, verbose=True)\n",
    "# model = model_upcast_utils.partially_upcast_nodes_to_fp32(ov_model.model, get_inputs_for_calibration(\"<human>: Which lakes are near Munich?\\n<bot>:\"))\n",
    "calibrated_model_dir = Path(f\"{model_dir}_calibrated_custom\")\n",
    "ov.save_model(model, calibrated_model_dir / \"openvino_model.xml\")\n",
    "shutil.copy(model_dir / \"config.json\", calibrated_model_dir / \"config.json\")\n",
    "# ov_model = OVModelForCausalLM.from_pretrained(calibrated_model_dir, device=device, ov_config=ov_config,\n",
    "#                                               config=AutoConfig.from_pretrained(calibrated_model_dir, trust_remote_code=True),\n",
    "#                                               trust_remote_code=True)\n",
    "ov_model.model = model\n",
    "ov_model._original_model = model\n",
    "ov_model.request = None\n",
    "ov_model.compile()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T14:42:39.579836900Z",
     "start_time": "2023-10-31T14:38:36.097849800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/guest/nsavelye/venvs/fp16_calibration/lib/python3.8/site-packages/optimum/intel/openvino/modeling_decoder.py:407: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which lakes are near Munich?\n",
      "<bot>: There is a total of 5 large and 3 small Bavarian Lakes. The largest lake, the Ammersee (German: \"Achse\" = 'the straight line'), has an area of about 50 km² with over 200 islands in its middle part; it's also one of Germany’s most popular tourist destinations because of its beautiful scenery as well as for water sports like sailing or rowing on Lake Constance which lies to the south-east across the border into"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import main\n",
    "importlib.reload(main)\n",
    "\n",
    "prompt = \"Which lakes are near Munich?\"\n",
    "\n",
    "generation_kwargs = dict(\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.1,\n",
    "        do_sample=0.1 > 0.0,\n",
    "        top_p=1.0,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "\n",
    "# print(run_generate(ov_model, prompt, model_configuration, **generation_kwargs))\n",
    "for text in main.run_generate(ov_model, tok, prompt, model_configuration, **generation_kwargs):\n",
    "    print(text, end=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T14:43:05.042019600Z",
     "start_time": "2023-10-31T14:42:39.580837800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T14:43:05.050177700Z",
     "start_time": "2023-10-31T14:43:05.043160800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
