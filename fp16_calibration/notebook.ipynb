{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-12T12:15:23.122085Z",
     "start_time": "2023-12-12T12:15:09.933150900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 21:22:51.783450: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-12 21:22:51.784569: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-12 21:22:51.805513: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-12 21:22:51.805955: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-12 21:22:52.153043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
      "Compiling the model to GPU ...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "ov_config = {'PERFORMANCE_HINT': 'LATENCY', 'NUM_STREAMS': '1', \"CACHE_DIR\": \"\"}\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "\n",
    "MODEL_ID = \"red-pajama-3b-chat\"\n",
    "# MODEL_ID = \"T5\"\n",
    "# MODEL_ID = \"tiny-sd-unet\"\n",
    "# MODEL_ID = \"codegen-2B-multi\"\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"tiny-sd-unet\", \"T5\"]:\n",
    "    half_type = \"f16\"\n",
    "    model_dir = models_dir / MODEL_ID / \"FP16\"\n",
    "    # model_dir = models_dir / MODEL_ID / \"FP16_calibrated\"\n",
    "    # model_dir = models_dir / MODEL_ID / \"INT8_compressed_weights\"\n",
    "    device = \"GPU\"\n",
    "    # device = \"CPU\"\n",
    "\n",
    "    if MODEL_ID == \"red-pajama-3b-chat\":\n",
    "        example_prompt = \"<human>: Which lakes are near Munich?\\\\n<bot>:\"\n",
    "    elif MODEL_ID == \"T5\":\n",
    "        example_prompt = \"ultra close color photo portrait of rainbow owl with deer horns in the woods\"\n",
    "    elif MODEL_ID == \"tiny-sd-unet\":\n",
    "        with open(\"unet_example_input.pkl\", \"rb\") as f:\n",
    "            unet_example_input = pickle.load(f)\n",
    "    else:\n",
    "        raise Exception(\"Unknown model\")\n",
    "elif MODEL_ID == \"codegen-2B-multi\":\n",
    "    half_type = \"bf16\"\n",
    "    model_dir = Path(\"/home/devuser/nsavelye/workspace/openvino.genai/llm_bench/python/codegen-2B-multi/pytorch/dldt/FP32\")\n",
    "    device = \"CPU\"\n",
    "    # ov_config[\"INFERENCE_PRECISION_HINT\"] = \"f32\"     # otherwise BF16 is used\n",
    "    example_prompt = \"# this function implement Fourier transform for imput array X\"\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\"]:\n",
    "    tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    ov_model_for_causal_lm = OVModelForCausalLM.from_pretrained(\n",
    "        model_dir, device=device, ov_config=ov_config,\n",
    "        config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True), trust_remote_code=True)\n",
    "    model = ov_model_for_causal_lm.model\n",
    "elif MODEL_ID == \"T5\":\n",
    "    model = core.read_model(model_dir / \"encoder_ir.xml\")\n",
    "elif MODEL_ID == \"tiny-sd-unet\":\n",
    "    model = core.read_model(model_dir / \"unet.xml\")\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 192/192 [15:48<00:00,  4.94s/it]\n",
      "Compiling the model to GPU ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQNR 0.10-quantile equals 33.79. Upcasted 20 of 192 considered nodes:\n",
      "__module.model.gpt_neox.layers.2.mlp.dense_4h_to_h/aten::linear/MatMul_766\n",
      "__module.model.gpt_neox.layers.3.mlp.dense_4h_to_h/aten::linear/MatMul_989\n",
      "__module.model.gpt_neox.layers.4.mlp.dense_4h_to_h/aten::linear/MatMul_1212\n",
      "__module.model.gpt_neox.layers.5.mlp.dense_4h_to_h/aten::linear/MatMul_1435\n",
      "__module.model.gpt_neox.layers.6.mlp.dense_4h_to_h/aten::linear/MatMul_1658\n",
      "__module.model.gpt_neox.layers.7.mlp.dense_4h_to_h/aten::linear/MatMul_1881\n",
      "__module.model.gpt_neox.layers.8.mlp.dense_4h_to_h/aten::linear/MatMul_2104\n",
      "__module.model.gpt_neox.layers.9.mlp.dense_4h_to_h/aten::linear/MatMul_2327\n",
      "__module.model.gpt_neox.layers.10.mlp.dense_4h_to_h/aten::linear/MatMul_2550\n",
      "__module.model.gpt_neox.layers.12.mlp.dense_4h_to_h/aten::linear/MatMul_2996\n",
      "__module.model.gpt_neox.layers.15.mlp.dense_4h_to_h/aten::linear/MatMul_3665\n",
      "__module.model.gpt_neox.layers.16.mlp.dense_4h_to_h/aten::linear/MatMul_3888\n",
      "__module.model.gpt_neox.layers.22.mlp.dense_4h_to_h/aten::linear/MatMul_5226\n",
      "__module.model.gpt_neox.layers.24.mlp.dense_4h_to_h/aten::linear/MatMul_5672\n",
      "__module.model.gpt_neox.layers.26.mlp.dense_4h_to_h/aten::linear/MatMul_6118\n",
      "__module.model.gpt_neox.layers.27.mlp.dense_4h_to_h/aten::linear/MatMul_6341\n",
      "__module.model.gpt_neox.layers.28.mlp.dense_4h_to_h/aten::linear/MatMul_6564\n",
      "__module.model.gpt_neox.layers.29.mlp.dense_4h_to_h/aten::linear/MatMul_6787\n",
      "__module.model.gpt_neox.layers.30.mlp.dense_4h_to_h/aten::linear/MatMul_7010\n",
      "__module.model.gpt_neox.layers.31.mlp.dense_4h_to_h/aten::linear/MatMul_7233\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import shutil\n",
    "import partially_upcast_nodes_to_fp32\n",
    "import model_upcast_utils\n",
    "import main\n",
    "importlib.reload(partially_upcast_nodes_to_fp32)\n",
    "importlib.reload(main)\n",
    "\n",
    "SAVE_MODEL = bool(1)\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\"]:\n",
    "    batch_size = -1\n",
    "    example_input = main.get_inputs_for_calibration(ov_model_for_causal_lm, tok, example_prompt)\n",
    "    if MODEL_ID == \"codegen-2B-multi\":\n",
    "        position_ids = np.cumsum(example_input[\"attention_mask\"], axis=1) - 1\n",
    "        position_ids[example_input[\"attention_mask\"] == 0] = 1\n",
    "        example_input[\"position_ids\"] = position_ids\n",
    "elif MODEL_ID == \"T5\":\n",
    "    batch_size = -1\n",
    "    # from diffusers import DiffusionPipeline\n",
    "    # tokenizer = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-M-v1.0\").tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models_dir / MODEL_ID / \"tokenizer\")\n",
    "    example_input = tokenizer(example_prompt, max_length=77, padding=\"max_length\", return_tensors=\"np\").input_ids\n",
    "elif MODEL_ID == \"tiny-sd-unet\":\n",
    "    batch_size = -1\n",
    "    example_input = unet_example_input\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")\n",
    "\n",
    "# shape_str = \"\"\n",
    "# for k, v in example_input.items():\n",
    "#     # np.save(f\"example_input/{k}.npy\", v.data)\n",
    "#     shape_str += f\"{k}{list(v.shape)},\".replace(' ', '')\n",
    "# print(shape_str)\n",
    "\n",
    "# upcasted_model = model_upcast_utils.partially_upcast_nodes_to_fp32(model, example_input)\n",
    "upcast_ratio = 0.1\n",
    "upcasted_model = partially_upcast_nodes_to_fp32.partially_upcast_nodes_to_fp32(\n",
    "    model, example_input, batch_size=-1, verbose=True, half_type=half_type, upcast_ratio=upcast_ratio)\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    calibrated_model_dir = Path(f\"{model_dir}_calibrated_{upcast_ratio:.2f}_matmuls_only\")\n",
    "    if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\"]:\n",
    "        # shutil.copytree(model_dir, calibrated_model_dir)\n",
    "        ov.save_model(upcasted_model, calibrated_model_dir / \"openvino_model.xml\")\n",
    "        for filename in [\"config.json\", \"added_tokens.json\", \"special_tokens_map.json\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\"]:\n",
    "            shutil.copy(str(model_dir / filename), str(calibrated_model_dir / filename))\n",
    "    elif MODEL_ID == \"T5\":\n",
    "        ov.save_model(upcasted_model, calibrated_model_dir / \"encoder_ir.xml\", compress_to_fp16=True)\n",
    "    elif MODEL_ID == \"tiny-sd-unet\":\n",
    "        ov.save_model(upcasted_model, calibrated_model_dir / \"unet.xml\")\n",
    "    else:\n",
    "        raise Exception(\"Unknown model\")\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\"]:\n",
    "    ov_model_for_causal_lm.model = upcasted_model\n",
    "    ov_model_for_causal_lm.request = None\n",
    "    ov_model_for_causal_lm.compile()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T12:31:22.059419400Z",
     "start_time": "2023-12-12T12:15:23.122085Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/guest/nsavelye/venvs/fp16_calibration/lib/python3.8/site-packages/optimum/intel/openvino/modeling_decoder.py:388: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which lakes are near Munich?\n",
      "<bot>: There is a total of 5 large and 3 medium-sized lakes in the vicinity. The largest lake, Lake Starnberg (Starnberger See), has an area of about 40 km² with over 100 islands; it's also one of Germany’s most popular bathing spots for both locals as well as tourists from all around Europe! Another famous destination nearby that offers great opportunities to swim or boat on its many canals: Isarsee – which means “Island Sea” due to numerous small islets located within this manmade reservoir created by damming up part of the river Isar). It covers approximately 50 square kilometres at full capacity but shrinks back down during dry periods when only 10% water remains behind after winter snowmelt runoff subsides… so be sure not to miss out if you visit between May and September especially since there will likely still plenty leftover even then!). Finally, another smaller body of fresh water called Tegernsee (“Tigerlake"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import main\n",
    "importlib.reload(main)\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\"]:\n",
    "    if MODEL_ID == \"red-pajama-3b-chat\":\n",
    "        prompt = \"Which lakes are near Munich?\"\n",
    "    else:\n",
    "        prompt = example_prompt\n",
    "    if MODEL_ID == \"red-pajama-3b-chat\":\n",
    "        generation_kwargs = dict(\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.1,\n",
    "            do_sample=0.1 > 0.0,\n",
    "            top_p=1.0,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    else:\n",
    "        generation_kwargs = dict(\n",
    "            max_new_tokens=100,\n",
    "            num_beams=1,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    # print(run_generate(ov_model, prompt, model_configuration, **generation_kwargs))\n",
    "    for text in main.run_generate(ov_model_for_causal_lm, tok, prompt, **generation_kwargs):\n",
    "        print(text, end=\"\")\n",
    "elif MODEL_ID == \"T5\":\n",
    "    from IPython.display import display\n",
    "    from deepfloyd_utils import TextEncoder, UnetFirstStage, pt_to_pil\n",
    "    from diffusers import DiffusionPipeline\n",
    "    import torch\n",
    "    import sys\n",
    "\n",
    "    sys.path.append(\"../notebooks/utils\")\n",
    "\n",
    "    prompt = 'ultra close color photo portrait of rainbow owl with deer horns in the woods'\n",
    "    negative_prompt = 'blurred unreal uncentered occluded'\n",
    "\n",
    "    RANDOM_SEED = 42\n",
    "    N_DIFFUSION_STEPS = 50\n",
    "    checkpoint_variant = 'fp16'\n",
    "    model_dtype = torch.float32\n",
    "\n",
    "    stage_1 = DiffusionPipeline.from_pretrained(\n",
    "        \"DeepFloyd/IF-I-M-v1.0\",\n",
    "        variant=checkpoint_variant,\n",
    "        torch_dtype=model_dtype\n",
    "    )\n",
    "\n",
    "    # Initialize TextEncoder wrapper class\n",
    "    stage_1.text_encoder = TextEncoder(calibrated_model_dir / \"encoder_ir_calibrated.xml\", dtype=model_dtype, device=device)\n",
    "\n",
    "    # Generate text embeddings\n",
    "    prompt_embeds, negative_embeds = stage_1.encode_prompt(prompt, negative_prompt=negative_prompt)\n",
    "\n",
    "    # Initialize the First Stage UNet wrapper class\n",
    "    stage_1.unet = UnetFirstStage(\n",
    "        \"/home/guest/nsavelye/workspace/fp16_calibration/notebooks/238-deepfloyd-if/models_new/unet_ir_I.xml\",\n",
    "        stage_1.unet.config,\n",
    "        dtype=model_dtype,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Fix PRNG seed\n",
    "    generator = torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "    # Inference\n",
    "    image = stage_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds,\n",
    "                    generator=generator, output_type=\"pt\", num_inference_steps=N_DIFFUSION_STEPS).images\n",
    "\n",
    "    # Show the image\n",
    "    display(pt_to_pil(image)[0])\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T12:32:02.303639Z",
     "start_time": "2023-12-12T12:31:22.059419400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:23:44.621647800Z",
     "start_time": "2023-12-04T14:23:44.621647800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
