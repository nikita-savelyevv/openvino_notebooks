{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-15T09:57:15.758506800Z",
     "start_time": "2023-12-15T09:57:12.962994900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 19:04:54.364076: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-15 19:04:54.365186: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-15 19:04:54.385515: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-15 19:04:54.385933: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-15 19:04:54.730152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "ov_config = {'PERFORMANCE_HINT': 'LATENCY', 'NUM_STREAMS': '1', \"CACHE_DIR\": \"\"}\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "\n",
    "# MODEL_ID = \"red-pajama-3b-chat\"\n",
    "# MODEL_ID = \"T5\"\n",
    "# MODEL_ID = \"tiny-sd-unet\"\n",
    "# MODEL_ID = \"codegen-2B-multi\"\n",
    "MODEL_ID = \"gpt-neox-20b\"\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"tiny-sd-unet\", \"T5\"]:\n",
    "    half_type = \"f16\"\n",
    "    model_dir = models_dir / MODEL_ID / \"FP16\"\n",
    "    # model_dir = models_dir / MODEL_ID / \"FP16_calibrated\"\n",
    "    # model_dir = models_dir / MODEL_ID / \"INT8_compressed_weights\"\n",
    "    device = \"GPU\"\n",
    "    # device = \"CPU\"\n",
    "\n",
    "    if MODEL_ID == \"red-pajama-3b-chat\":\n",
    "        example_prompt = \"<human>: Which lakes are near Munich?\\\\n<bot>:\"\n",
    "    elif MODEL_ID == \"T5\":\n",
    "        example_prompt = \"ultra close color photo portrait of rainbow owl with deer horns in the woods\"\n",
    "    elif MODEL_ID == \"tiny-sd-unet\":\n",
    "        with open(\"unet_example_input.pkl\", \"rb\") as f:\n",
    "            unet_example_input = pickle.load(f)\n",
    "    else:\n",
    "        raise Exception(\"Unknown model\")\n",
    "elif MODEL_ID in [\"codegen-2B-multi\", \"gpt-neox-20b\"]:\n",
    "    half_type = \"bf16\"\n",
    "    device = \"CPU\"\n",
    "    # ov_config[\"INFERENCE_PRECISION_HINT\"] = \"f32\"     # otherwise BF16 is used\n",
    "    if MODEL_ID == \"codegen-2B-multi\":\n",
    "        model_dir = Path(\"/home/devuser/nsavelye/workspace/openvino.genai/llm_bench/python/codegen-2B-multi/pytorch/dldt/FP32\")\n",
    "        example_prompt = \"# this function implement Fourier transform for imput array X\"\n",
    "    elif MODEL_ID == \"gpt-neox-20b\":\n",
    "        model_dir = Path(\"/home/devuser/nsavelye/workspace/openvino.genai/llm_bench/python/gpt-neox-20b/fp16/pytorch/dldt/FP16\")\n",
    "        example_prompt = \"Which lakes are near Munich?\"\n",
    "    else:\n",
    "        raise Exception(\"Unknown model\")\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\", \"gpt-neox-20b\"]:\n",
    "    tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    ov_model_for_causal_lm = OVModelForCausalLM.from_pretrained(\n",
    "        model_dir, device=device, ov_config=ov_config,\n",
    "        config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True), trust_remote_code=True)\n",
    "    model = ov_model_for_causal_lm.model\n",
    "elif MODEL_ID == \"T5\":\n",
    "    model = core.read_model(model_dir / \"encoder_ir.xml\")\n",
    "elif MODEL_ID == \"tiny-sd-unet\":\n",
    "    model = core.read_model(model_dir / \"unet.xml\")\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:34<00:00, 34.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQNR 0.50-quantile equals 44.02. Upcasted 25 of 50 considered nodes:\n",
      "__module.down_blocks.0.resnets.0.conv1/aten::_convolution/Convolution\n",
      "__module.down_blocks.0.resnets.0.conv2/aten::_convolution/Convolution\n",
      "__module.down_blocks.0.downsamplers.0.conv/aten::_convolution/Convolution\n",
      "__module.down_blocks.1.resnets.0.conv1/aten::_convolution/Convolution\n",
      "__module.down_blocks.1.resnets.0.conv2/aten::_convolution/Convolution\n",
      "__module.down_blocks.1.downsamplers.0.conv/aten::_convolution/Convolution\n",
      "__module.down_blocks.2.resnets.0.conv1/aten::_convolution/Convolution\n",
      "__module.down_blocks.2.resnets.0.conv2/aten::_convolution/Convolution\n",
      "__module.up_blocks.0.resnets.0.conv_shortcut/aten::_convolution/Convolution\n",
      "__module.up_blocks.0.resnets.0.conv1/aten::_convolution/Convolution\n",
      "__module.up_blocks.0.resnets.0.conv2/aten::_convolution/Convolution\n",
      "__module.up_blocks.0.resnets.1.conv_shortcut/aten::_convolution/Convolution\n",
      "__module.up_blocks.0.resnets.1.conv1/aten::_convolution/Convolution\n",
      "__module.up_blocks.0.resnets.1.conv2/aten::_convolution/Convolution\n",
      "__module.up_blocks.0.upsamplers.0.conv/aten::_convolution/Convolution\n",
      "__module.up_blocks.1.resnets.0.conv_shortcut/aten::_convolution/Convolution\n",
      "__module.up_blocks.1.resnets.0.conv1/aten::_convolution/Convolution\n",
      "__module.up_blocks.1.resnets.0.conv2/aten::_convolution/Convolution\n",
      "__module.up_blocks.1.resnets.1.conv1/aten::_convolution/Convolution\n",
      "__module.up_blocks.1.resnets.1.conv2/aten::_convolution/Convolution\n",
      "__module.up_blocks.1.upsamplers.0.conv/aten::_convolution/Convolution\n",
      "__module.up_blocks.2.resnets.0.conv1/aten::_convolution/Convolution\n",
      "__module.up_blocks.2.resnets.0.conv2/aten::_convolution/Convolution\n",
      "__module.up_blocks.2.resnets.1.conv1/aten::_convolution/Convolution\n",
      "__module.up_blocks.2.resnets.1.conv2/aten::_convolution/Convolution\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import shutil\n",
    "import partially_upcast_nodes_to_fp32\n",
    "import model_upcast_utils\n",
    "import main\n",
    "importlib.reload(partially_upcast_nodes_to_fp32)\n",
    "importlib.reload(main)\n",
    "\n",
    "SAVE_MODEL = bool(1)\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\", \"gpt-neox-20b\"]:\n",
    "    batch_size = -1\n",
    "    example_input = main.get_inputs_for_calibration(ov_model_for_causal_lm, tok, example_prompt)\n",
    "    if MODEL_ID in [\"codegen-2B-multi\", \"gpt-neox-20b\"]:\n",
    "        position_ids = np.cumsum(example_input[\"attention_mask\"], axis=1) - 1\n",
    "        position_ids[example_input[\"attention_mask\"] == 0] = 1\n",
    "        example_input[\"position_ids\"] = position_ids\n",
    "elif MODEL_ID == \"T5\":\n",
    "    batch_size = -1\n",
    "    # from diffusers import DiffusionPipeline\n",
    "    # tokenizer = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-M-v1.0\").tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models_dir / MODEL_ID / \"tokenizer\")\n",
    "    example_input = tokenizer(example_prompt, max_length=77, padding=\"max_length\", return_tensors=\"np\").input_ids\n",
    "elif MODEL_ID == \"tiny-sd-unet\":\n",
    "    batch_size = -1\n",
    "    example_input = unet_example_input\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")\n",
    "\n",
    "# shape_str = \"\"\n",
    "# for k, v in example_input.items():\n",
    "#     # np.save(f\"example_input/{k}.npy\", v.data)\n",
    "#     shape_str += f\"{k}{list(v.shape)},\".replace(' ', '')\n",
    "# print(shape_str)\n",
    "\n",
    "# upcasted_model = model_upcast_utils.partially_upcast_nodes_to_fp32(model, example_input)\n",
    "upcast_ratio = 1.0\n",
    "upcasted_model = partially_upcast_nodes_to_fp32.partially_upcast_nodes_to_fp32(\n",
    "    model, example_input, batch_size=-1, verbose=True, half_type=half_type, upcast_ratio=upcast_ratio)\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    calibrated_model_dir = Path(f\"{model_dir}_calibrated_{upcast_ratio:.2f}\")\n",
    "    if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\", \"gpt-neox-20b\"]:\n",
    "        # shutil.copytree(model_dir, calibrated_model_dir)\n",
    "        ov.save_model(upcasted_model, calibrated_model_dir / \"openvino_model.xml\")\n",
    "        for filename in [\"config.json\", \"added_tokens.json\", \"special_tokens_map.json\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\"]:\n",
    "            if (model_dir / filename).exists():\n",
    "                shutil.copy(str(model_dir / filename), str(calibrated_model_dir / filename))\n",
    "    elif MODEL_ID == \"T5\":\n",
    "        ov.save_model(upcasted_model, calibrated_model_dir / \"encoder_ir.xml\", compress_to_fp16=True)\n",
    "    elif MODEL_ID == \"tiny-sd-unet\":\n",
    "        ov.save_model(upcasted_model, calibrated_model_dir / \"unet.xml\")\n",
    "    else:\n",
    "        raise Exception(\"Unknown model\")\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\", \"gpt-neox-20b\"]:\n",
    "    ov_model_for_causal_lm.model = upcasted_model\n",
    "    ov_model_for_causal_lm.request = None\n",
    "    ov_model_for_causal_lm.compile()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T09:57:50.069916600Z",
     "start_time": "2023-12-15T09:57:15.758506800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unknown model",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 80\u001B[0m\n\u001B[1;32m     78\u001B[0m     display(pt_to_pil(image)[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 80\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mException\u001B[0m: Unknown model"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import main\n",
    "importlib.reload(main)\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\", \"gpt-neox-20b\"]:\n",
    "    if MODEL_ID in [\"red-pajama-3b-chat\", \"gpt-neox-20b\"]:\n",
    "        prompt = \"Which lakes are near Munich?\"\n",
    "    elif MODEL_ID == \"codegen-2B-multi\":\n",
    "        prompt = example_prompt\n",
    "    else:\n",
    "        raise Exception(\"Unknown model\")\n",
    "    if MODEL_ID == \"red-pajama-3b-chat\":\n",
    "        generation_kwargs = dict(\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.1,\n",
    "            do_sample=0.1 > 0.0,\n",
    "            top_p=1.0,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    elif MODEL_ID in [\"codegen-2B-multi\", \"gpt-neox-20b\"]:\n",
    "        generation_kwargs = dict(\n",
    "            max_new_tokens=100,\n",
    "            num_beams=1,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Unknown model\")\n",
    "\n",
    "    # print(run_generate(ov_model, prompt, model_configuration, **generation_kwargs))\n",
    "    for text in main.run_generate(ov_model_for_causal_lm, tok, prompt, **generation_kwargs):\n",
    "        print(text, end=\"\")\n",
    "elif MODEL_ID == \"T5\":\n",
    "    from IPython.display import display\n",
    "    from deepfloyd_utils import TextEncoder, UnetFirstStage, pt_to_pil\n",
    "    from diffusers import DiffusionPipeline\n",
    "    import torch\n",
    "    import sys\n",
    "\n",
    "    sys.path.append(\"../notebooks/utils\")\n",
    "\n",
    "    prompt = 'ultra close color photo portrait of rainbow owl with deer horns in the woods'\n",
    "    negative_prompt = 'blurred unreal uncentered occluded'\n",
    "\n",
    "    RANDOM_SEED = 42\n",
    "    N_DIFFUSION_STEPS = 50\n",
    "    checkpoint_variant = 'fp16'\n",
    "    model_dtype = torch.float32\n",
    "\n",
    "    stage_1 = DiffusionPipeline.from_pretrained(\n",
    "        \"DeepFloyd/IF-I-M-v1.0\",\n",
    "        variant=checkpoint_variant,\n",
    "        torch_dtype=model_dtype\n",
    "    )\n",
    "\n",
    "    # Initialize TextEncoder wrapper class\n",
    "    stage_1.text_encoder = TextEncoder(calibrated_model_dir / \"encoder_ir_calibrated.xml\", dtype=model_dtype, device=device)\n",
    "\n",
    "    # Generate text embeddings\n",
    "    prompt_embeds, negative_embeds = stage_1.encode_prompt(prompt, negative_prompt=negative_prompt)\n",
    "\n",
    "    # Initialize the First Stage UNet wrapper class\n",
    "    stage_1.unet = UnetFirstStage(\n",
    "        \"/home/guest/nsavelye/workspace/fp16_calibration/notebooks/238-deepfloyd-if/models_new/unet_ir_I.xml\",\n",
    "        stage_1.unet.config,\n",
    "        dtype=model_dtype,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Fix PRNG seed\n",
    "    generator = torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "    # Inference\n",
    "    image = stage_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds,\n",
    "                    generator=generator, output_type=\"pt\", num_inference_steps=N_DIFFUSION_STEPS).images\n",
    "\n",
    "    # Show the image\n",
    "    display(pt_to_pil(image)[0])\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T09:32:33.580571100Z",
     "start_time": "2023-12-14T09:32:33.566244400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-12T14:33:21.796563400Z",
     "start_time": "2023-12-12T14:33:21.795563400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
