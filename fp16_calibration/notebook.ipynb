{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-06T09:36:53.759982600Z",
     "start_time": "2023-12-06T09:36:44.037343600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The argument `trust_remote_code` is to be used along with export=True. It will be ignored.\n",
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "ov_config = {'PERFORMANCE_HINT': 'LATENCY', 'NUM_STREAMS': '1', \"CACHE_DIR\": \"\"}\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "\n",
    "# MODEL_ID = \"red-pajama-3b-chat\"\n",
    "# MODEL_ID = \"T5\"\n",
    "# MODEL_ID = \"tiny-sd-unet\"\n",
    "MODEL_ID = \"codegen-2B-multi\"\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"tiny-sd-unet\", \"T5\"]:\n",
    "    half_type = \"f16\"\n",
    "    model_dir = models_dir / MODEL_ID / \"FP16\"\n",
    "    # model_dir = models_dir / MODEL_ID / \"FP16_calibrated\"\n",
    "    # model_dir = models_dir / MODEL_ID / \"INT8_compressed_weights\"\n",
    "    device = \"GPU\"\n",
    "    # device = \"CPU\"\n",
    "\n",
    "    if MODEL_ID == \"red-pajama-3b-chat\":\n",
    "        example_prompt = \"<human>: Which lakes are near Munich?\\\\n<bot>:\"\n",
    "    elif MODEL_ID == \"T5\":\n",
    "        example_prompt = \"ultra close color photo portrait of rainbow owl with deer horns in the woods\"\n",
    "    elif MODEL_ID == \"tiny-sd-unet\":\n",
    "        with open(\"unet_example_input.pkl\", \"rb\") as f:\n",
    "            unet_example_input = pickle.load(f)\n",
    "    else:\n",
    "        raise Exception(\"Unknown model\")\n",
    "elif MODEL_ID == \"codegen-2B-multi\":\n",
    "    half_type = \"bf16\"\n",
    "    model_dir = Path(\"/home/devuser/nsavelye/workspace/openvino.genai/llm_bench/python/codegen-2B-multi/pytorch/dldt/FP32\")\n",
    "    device = \"CPU\"\n",
    "    # ov_config[\"INFERENCE_PRECISION_HINT\"] = \"f32\"     # otherwise BF16 is used\n",
    "    example_prompt = \"# this function implement Fourier transform for imput array X\"\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\"]:\n",
    "    tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    ov_model_for_causal_lm = OVModelForCausalLM.from_pretrained(\n",
    "        model_dir, device=device, ov_config=ov_config,\n",
    "        config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True), trust_remote_code=True)\n",
    "    model = ov_model_for_causal_lm.model\n",
    "elif MODEL_ID == \"T5\":\n",
    "    model = core.read_model(model_dir / \"encoder_ir.xml\")\n",
    "elif MODEL_ID == \"tiny-sd-unet\":\n",
    "    model = core.read_model(model_dir / \"unet.xml\")\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import shutil\n",
    "import partially_upcast_nodes_to_fp32\n",
    "import model_upcast_utils\n",
    "import main\n",
    "importlib.reload(partially_upcast_nodes_to_fp32)\n",
    "importlib.reload(main)\n",
    "\n",
    "SAVE_MODEL = bool(1)\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\"]:\n",
    "    batch_size = 50\n",
    "    example_input = main.get_inputs_for_calibration(ov_model_for_causal_lm, tok, example_prompt)\n",
    "    if MODEL_ID == \"codegen-2B-multi\":\n",
    "        position_ids = np.cumsum(example_input[\"attention_mask\"], axis=1) - 1\n",
    "        position_ids[example_input[\"attention_mask\"] == 0] = 1\n",
    "        example_input[\"position_ids\"] = position_ids\n",
    "elif MODEL_ID == \"T5\":\n",
    "    batch_size = 50\n",
    "    # from diffusers import DiffusionPipeline\n",
    "    # tokenizer = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-M-v1.0\").tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models_dir / MODEL_ID / \"tokenizer\")\n",
    "    example_input = tokenizer(example_prompt, max_length=77, padding=\"max_length\", return_tensors=\"np\").input_ids\n",
    "elif MODEL_ID == \"tiny-sd-unet\":\n",
    "    batch_size = -1\n",
    "    example_input = unet_example_input\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")\n",
    "\n",
    "# shape_str = \"\"\n",
    "# for k, v in example_input.items():\n",
    "#     # np.save(f\"example_input/{k}.npy\", v.data)\n",
    "#     shape_str += f\"{k}{list(v.shape)},\".replace(' ', '')\n",
    "# print(shape_str)\n",
    "\n",
    "# upcasted_model = model_upcast_utils.partially_upcast_nodes_to_fp32(model, example_input)\n",
    "upcast_ratio = 0.9\n",
    "upcasted_model = partially_upcast_nodes_to_fp32.partially_upcast_nodes_to_fp32(\n",
    "    model, example_input, batch_size=batch_size, verbose=True, half_type=half_type, upcast_ratio=upcast_ratio)\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    calibrated_model_dir = Path(f\"{model_dir}_calibrated_{upcast_ratio:.2f}\")\n",
    "    if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\"]:\n",
    "        # shutil.copytree(model_dir, calibrated_model_dir)\n",
    "        ov.save_model(upcasted_model, calibrated_model_dir / \"openvino_model.xml\")\n",
    "        for filename in [\"config.json\", \"added_tokens.json\", \"special_tokens_map.json\", \"tokenizer.json\", \"tokenizer_config.json\", \"vocab.json\"]:\n",
    "            shutil.copy(str(model_dir / filename), str(calibrated_model_dir / filename))\n",
    "    elif MODEL_ID == \"T5\":\n",
    "        ov.save_model(upcasted_model, calibrated_model_dir / \"encoder_ir_calibrated.xml\", compress_to_fp16=True)\n",
    "    elif MODEL_ID == \"tiny-sd-unet\":\n",
    "        ov.save_model(upcasted_model, calibrated_model_dir / \"unet_calibrated.xml\")\n",
    "    else:\n",
    "        raise Exception(\"Unknown model\")\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\"]:\n",
    "    ov_model_for_causal_lm.model = upcasted_model\n",
    "    ov_model_for_causal_lm.request = None\n",
    "    ov_model_for_causal_lm.compile()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-06T09:36:38.544921Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/devuser/nsavelye/venvs/fp16_calibration/lib/python3.8/site-packages/optimum/intel/openvino/modeling_decoder.py:388: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# this function implement Fourier transform for imput array X\n",
      "    # and output array Y\n",
      "    #\n",
      "    # X: input array\n",
      "    # Y: output array\n",
      "    #\n",
      "    # return:\n",
      "    #   0\n",
      "\n",
      "\n",
      "(function () {\n",
      "    var i, j, k,\n",
      "        X = [],\n",
      "        Y = [],\n",
      "        N = X.length,\n",
      "        M = Y.length,\n",
      "        N2 = N / 2,\n",
      "        M2 = M / 2,\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import main\n",
    "importlib.reload(main)\n",
    "\n",
    "if MODEL_ID in [\"red-pajama-3b-chat\", \"codegen-2B-multi\"]:\n",
    "    if MODEL_ID == \"red-pajama-3b-chat\":\n",
    "        prompt = \"Which lakes are near Munich?\"\n",
    "    else:\n",
    "        prompt = example_prompt\n",
    "    if MODEL_ID == \"red-pajama-3b-chat\":\n",
    "        generation_kwargs = dict(\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.1,\n",
    "            do_sample=0.1 > 0.0,\n",
    "            top_p=1.0,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "    else:\n",
    "        generation_kwargs = dict(\n",
    "            max_new_tokens=100,\n",
    "            num_beams=1,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    # print(run_generate(ov_model, prompt, model_configuration, **generation_kwargs))\n",
    "    for text in main.run_generate(ov_model_for_causal_lm, tok, prompt, **generation_kwargs):\n",
    "        print(text, end=\"\")\n",
    "elif MODEL_ID == \"T5\":\n",
    "    from IPython.display import display\n",
    "    from deepfloyd_utils import TextEncoder, UnetFirstStage, pt_to_pil\n",
    "    from diffusers import DiffusionPipeline\n",
    "    import torch\n",
    "    import sys\n",
    "\n",
    "    sys.path.append(\"../notebooks/utils\")\n",
    "\n",
    "    prompt = 'ultra close color photo portrait of rainbow owl with deer horns in the woods'\n",
    "    negative_prompt = 'blurred unreal uncentered occluded'\n",
    "\n",
    "    RANDOM_SEED = 42\n",
    "    N_DIFFUSION_STEPS = 50\n",
    "    checkpoint_variant = 'fp16'\n",
    "    model_dtype = torch.float32\n",
    "\n",
    "    stage_1 = DiffusionPipeline.from_pretrained(\n",
    "        \"DeepFloyd/IF-I-M-v1.0\",\n",
    "        variant=checkpoint_variant,\n",
    "        torch_dtype=model_dtype\n",
    "    )\n",
    "\n",
    "    # Initialize TextEncoder wrapper class\n",
    "    stage_1.text_encoder = TextEncoder(calibrated_model_dir / \"encoder_ir_calibrated.xml\", dtype=model_dtype, device=device)\n",
    "\n",
    "    # Generate text embeddings\n",
    "    prompt_embeds, negative_embeds = stage_1.encode_prompt(prompt, negative_prompt=negative_prompt)\n",
    "\n",
    "    # Initialize the First Stage UNet wrapper class\n",
    "    stage_1.unet = UnetFirstStage(\n",
    "        \"/home/guest/nsavelye/workspace/fp16_calibration/notebooks/238-deepfloyd-if/models_new/unet_ir_I.xml\",\n",
    "        stage_1.unet.config,\n",
    "        dtype=model_dtype,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Fix PRNG seed\n",
    "    generator = torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "    # Inference\n",
    "    image = stage_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds,\n",
    "                    generator=generator, output_type=\"pt\", num_inference_steps=N_DIFFUSION_STEPS).images\n",
    "\n",
    "    # Show the image\n",
    "    display(pt_to_pil(image)[0])\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T09:37:14.026233100Z",
     "start_time": "2023-12-06T09:36:56.363723800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T14:23:44.621647800Z",
     "start_time": "2023-12-04T14:23:44.621647800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
