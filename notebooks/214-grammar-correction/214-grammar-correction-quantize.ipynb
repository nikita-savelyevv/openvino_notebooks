{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b677ff-3399-4f27-ac0f-782bfe25f151",
   "metadata": {},
   "source": [
    "# Post-Training Quantization of Grammar Error Correction model with NNCF\n",
    "\n",
    "The goal of this tutorial is to demonstrate how to speed up the model by applying 8-bit post-training quantization from [NNCF](https://github.com/openvinotoolkit/nncf/) (Neural Network Compression Framework) and infer quantized model via OpenVINOâ„¢ Toolkit. The optimization process contains the following steps:\n",
    "\n",
    "1. Quantize the converted OpenVINO model from [214-grammar-correction-convert notebook](214-grammar-correction-convert.ipynb) with NNCF.\n",
    "2. Check model result for the sample text.\n",
    "3. Compare model size, performance and accuracy of FP32 and quantized INT8 models.\n",
    "\n",
    "> **NOTE**: you should run [214-grammar-correction-convert](214-grammar-correction-convert.ipynb) notebook first to generate OpenVINO IR model that is used for quantization.\n",
    "\n",
    "#### Table of contents:\n",
    "- [Prerequisites](#Prerequisites-$\\Uparrow$)\n",
    "- [Quantization](#Quantization-$\\Uparrow$)\n",
    "    - [Prepare calibration dataset](#Prepare-calibration-dataset-$\\Uparrow$)\n",
    "    - [Run quantization](#Run-quantization-$\\Uparrow$)\n",
    "- [Run grammar correction with quantized OpenVINO model](#Run-grammar-correction-with-quantized-OpenVINO-model-$\\Uparrow$)\n",
    "- [Compare model size, performance and accuracy](#Compare-model-size,-performance-and-accuracy-$\\Uparrow$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a760a-aaf7-41f6-ab0d-da993e486336",
   "metadata": {},
   "source": [
    "## Prerequisites [$\\Uparrow$](#Table-of-contents:)\n",
    "\n",
    "First we define the prerequisites and load models same as in [214-grammar-correction-convert](214-grammar-correction-convert.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ef5063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:21:23.543149500Z",
     "start_time": "2023-09-18T19:21:23.467432600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"git+https://github.com/openvinotoolkit/nncf.git@9c671f0ae0a118e4bc2de8b09e66425931c0bfa4\"\n",
    "%pip install -q datasets\n",
    "%pip install -q jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e0871-c828-4104-a986-230a27c913a5",
   "metadata": {},
   "source": [
    "Select inference device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "053b4f68-a329-43ac-920c-9d86949edc05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:21:24.008366300Z",
     "start_time": "2023-09-18T19:21:23.534302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810e8dbdbe584ce79a123dd3ddcd0c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131a0ec-654e-435e-a668-55ad33cff74b",
   "metadata": {},
   "source": [
    "Load Grammar Checker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47af0ecf-99ff-4852-bfaa-6692caeaca21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:21:29.497444700Z",
     "start_time": "2023-09-18T19:21:24.013482700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda-11.7'\n",
      "2023-09-20 18:27:16.343735: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-20 18:27:16.378509: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-20 18:27:17.025823: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/nsavel/venvs/ov_notebooks_tmp/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Compiling the model...\n",
      "Set CACHE_DIR to roberta-base-cola/model_cache\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForSequenceClassification\n",
    "\n",
    "grammar_checker_model_id = \"textattack/roberta-base-CoLA\"\n",
    "grammar_checker_dir = Path(\"roberta-base-cola\")\n",
    "grammar_checker_tokenizer = AutoTokenizer.from_pretrained(grammar_checker_model_id)\n",
    "\n",
    "if grammar_checker_dir.exists():\n",
    "    grammar_checker_model = OVModelForSequenceClassification.from_pretrained(grammar_checker_dir, device=device.value)\n",
    "else:\n",
    "    grammar_checker_model = OVModelForSequenceClassification.from_pretrained(grammar_checker_model_id, export=True, device=device.value)\n",
    "    grammar_checker_model.save_pretrained(grammar_checker_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba3c17-9f94-4d1c-afae-39c857caf5af",
   "metadata": {},
   "source": [
    "Load Grammar Corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dcf0e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:21:39.032412600Z",
     "start_time": "2023-09-18T19:21:29.498569900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the encoder...\n",
      "Compiling the decoder...\n",
      "Compiling the decoder...\n"
     ]
    }
   ],
   "source": [
    "from optimum.intel.openvino import OVModelForSeq2SeqLM\n",
    "\n",
    "grammar_corrector_model_id = \"pszemraj/flan-t5-large-grammar-synthesis\"\n",
    "grammar_corrector_dir = Path(\"flan-t5-large-grammar-synthesis\")\n",
    "grammar_corrector_tokenizer = AutoTokenizer.from_pretrained(grammar_corrector_model_id)\n",
    "\n",
    "if grammar_corrector_dir.exists():\n",
    "    grammar_corrector_model_fp32 = OVModelForSeq2SeqLM.from_pretrained(grammar_corrector_dir, device=device.value)\n",
    "else:\n",
    "    grammar_corrector_model_fp32 = OVModelForSeq2SeqLM.from_pretrained(grammar_corrector_model_id, export=True, device=device.value)\n",
    "    grammar_corrector_model_fp32.save_pretrained(grammar_corrector_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7679a7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Create grammar checker and corrector pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf3d0d24-c94a-42c7-b603-499bd9d251d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:21:39.040861500Z",
     "start_time": "2023-09-18T19:21:39.032412600Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "grammar_checker_pipe = pipeline(\"text-classification\", model=grammar_checker_model, tokenizer=grammar_checker_tokenizer)\n",
    "grammar_corrector_pipe_fp32 = pipeline(\"text2text-generation\", model=grammar_corrector_model_fp32, tokenizer=grammar_corrector_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69faa673-45fd-481d-9573-4f54ea17fb77",
   "metadata": {},
   "source": [
    "## Quantization [$\\Uparrow$](#Table-of-contents:)\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf/) enables post-training quantization by adding the quantization layers into the model graph and then using a subset of the training dataset to initialize the parameters of these additional quantization layers. Quantized operations are executed in `INT8` instead of `FP32`/`FP16` making model inference faster.\n",
    "\n",
    "Grammar checker model takes up a tiny portion of the whole text correction pipeline so we optimize only the grammar corrector model. Grammar corrector itself consists of three models: encoder, first call decoder and decoder with past. The last model's share of inference time is about 90%. Because of this we quantize only it.\n",
    "\n",
    "The optimization process contains the following step:\n",
    "\n",
    "1. Create a calibration dataset for quantization.\n",
    "2. Run `nncf.quantize` to obtain quantized models.\n",
    "3. Serialize the `INT8` model using `openvino.runtime.serialize` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7dc322",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Prepare calibration dataset [$\\Uparrow$](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3733515-6a3d-48e4-8f8f-9e731ab28b1b",
   "metadata": {},
   "source": [
    "In order to collect calibration dataset for the decoder model we need to collect tensors which are used at its inputs. For this, we wrap one of the methods that is used during its inference. The wrapper intercepts input tensors and collects them in a separate `calibration_data` list. Thus, after we inference grammar corrector on some text samples this list will contain input data for quantization of this model.\n",
    "\n",
    "We use first several samples from validation split of [jfleg](https://huggingface.co/datasets/jfleg) text correction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2cb9ec-8886-4ed5-b2fc-f9733ba93ea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:22:05.099076Z",
     "start_time": "2023-09-18T19:21:39.040861500Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from contextlib import contextmanager\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "COLLECT_CALIBRATION_DATA = False\n",
    "calibration_data = []\n",
    "ov_decoder = grammar_corrector_pipe_fp32.model.decoder_with_past\n",
    "\n",
    "@contextmanager\n",
    "def calibration_data_collection():\n",
    "    global COLLECT_CALIBRATION_DATA\n",
    "    try:\n",
    "        COLLECT_CALIBRATION_DATA = True\n",
    "        yield\n",
    "    finally:\n",
    "        COLLECT_CALIBRATION_DATA = False\n",
    "\n",
    "def wrap_for_data_collection():\n",
    "    original_fn = ov_decoder.request.start_async\n",
    "    def wrapper(*args, **kwargs):\n",
    "        inputs = kwargs.get(\"inputs\", args[0])\n",
    "        if COLLECT_CALIBRATION_DATA:\n",
    "            calibration_data.append(inputs)\n",
    "        return original_fn(*args, **kwargs)\n",
    "    ov_decoder.request.start_async = wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5f8265-a8ba-4708-b66b-e0b7cd705e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afc6eb4d0e840fe8bf7977a7810499c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting calibration data:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsavel/venvs/ov_notebooks_tmp/lib/python3.8/site-packages/optimum/intel/openvino/modeling_seq2seq.py:339: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  last_hidden_state = torch.from_numpy(self.request(inputs, shared_memory=True)[\"last_hidden_state\"]).to(\n",
      "/home/nsavel/venvs/ov_notebooks_tmp/lib/python3.8/site-packages/optimum/intel/openvino/modeling_seq2seq.py:416: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n",
      "/tmp/ipykernel_20786/4225757429.py:24: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  return original_fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "CALIBRATION_DATASET_SIZE = 10\n",
    "\n",
    "wrap_for_data_collection()\n",
    "\n",
    "calibration_dataset = datasets.load_dataset(\"jfleg\", split=f\"validation[:{CALIBRATION_DATASET_SIZE}]\")\n",
    "with calibration_data_collection():\n",
    "    for data_item in tqdm(calibration_dataset, total=CALIBRATION_DATASET_SIZE, desc=\"Collecting calibration data\"):\n",
    "        grammar_corrector_pipe_fp32(data_item[\"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf4f7b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Run quantization [$\\Uparrow$](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88f3849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:26:46.375979600Z",
     "start_time": "2023-09-18T19:22:05.101557800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9195e7ba63074447a4f0b585c318a37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebac1bc5a2c2437683b4294caece1c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cf00adef6541a6a3d960233c8fe739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openvino.runtime as ov\n",
    "import nncf\n",
    "from nncf.quantization.range_estimator import RangeEstimatorParameters, StatisticsCollectorParameters, StatisticsType\n",
    "\n",
    "quantized_model_path = Path(\"quantized_decoder_with_past\") / \"openvino_model.xml\"\n",
    "\n",
    "quantized_model = nncf.quantize(\n",
    "    ov_decoder.model,\n",
    "    calibration_dataset=nncf.Dataset(calibration_data),\n",
    "    subset_size=len(calibration_data),\n",
    "    model_type=nncf.ModelType.TRANSFORMER,\n",
    "    advanced_parameters=nncf.AdvancedQuantizationParameters(\n",
    "        disable_bias_correction=True,  # Disable bias correction because the model does not contain quantizable operations with bias\n",
    "        smooth_quant_alpha=0.95,  # The value of 0.95 was selected by grid search\n",
    "        activations_range_estimator_params=RangeEstimatorParameters(\n",
    "            # Quantile statistic is employed due to outliers in some activations; this parameter was found by quantize_with_accuracy_control method\n",
    "            max=StatisticsCollectorParameters(StatisticsType.QUANTILE)\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "if not quantized_model_path.parent.exists():\n",
    "    quantized_model_path.parent.mkdir(parents=True)\n",
    "ov.serialize(quantized_model, quantized_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55fe244-011a-4b99-9a90-867fb2cceb8b",
   "metadata": {},
   "source": [
    "## Run grammar correction with quantized OpenVINO model [$\\Uparrow$](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc3786",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Create quantized grammar corrector pipeline and run correction based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a9d7b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:27:06.077587800Z",
     "start_time": "2023-09-18T19:26:46.375979600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the encoder...\n",
      "Compiling the decoder...\n",
      "Compiling the decoder...\n",
      "Compiling the decoder...\n"
     ]
    }
   ],
   "source": [
    "grammar_corrector_model_int8 = OVModelForSeq2SeqLM.from_pretrained(grammar_corrector_dir, device=device.value)\n",
    "grammar_corrector_model_int8.decoder_with_past.model = quantized_model\n",
    "grammar_corrector_model_int8.decoder_with_past.request = None\n",
    "grammar_corrector_model_int8.decoder_with_past._compile()\n",
    "grammar_corrector_pipe_int8 = pipeline(\"text2text-generation\", model=grammar_corrector_model_int8, tokenizer=grammar_corrector_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee397f5-12cb-460b-8824-327f19af8e5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:27:08.888308300Z",
     "start_time": "2023-09-18T19:27:06.080702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2154ac33e484b64a1a87ae41dc4de75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "correcting text..:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20786/4225757429.py:24: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  return original_fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5607560144a44cbcb2a241bd3662f8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "correcting text..:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import correct_text\n",
    "\n",
    "default_text = (\n",
    "    \"Most of the course is about semantic or  content of language but there are also interesting\"\n",
    "    \" topics to be learned from the servicefeatures except statistics in characters in documents.At\"\n",
    "    \" this point, He introduces herself as his native English speaker and goes on to say that if\"\n",
    "    \" you contine to work on social scnce\"\n",
    ")\n",
    "\n",
    "corrected_text_fp32 = correct_text(default_text, grammar_checker_pipe, grammar_corrector_pipe_fp32)\n",
    "corrected_text_int8 = correct_text(default_text, grammar_checker_pipe, grammar_corrector_pipe_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e60cb86-eff4-4172-9f36-049329e1cbe3",
   "metadata": {},
   "source": [
    "Let's see the results. The generated texts for quantized `INT8` model and original `FP32` model should be almost the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5862ec36-8d77-418f-9295-5dc644b50068",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:27:08.888308300Z",
     "start_time": "2023-09-18T19:27:08.874629400Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:                   Most of the course is about semantic or  content of language but there are also interesting topics to be learned from the servicefeatures except statistics in characters in documents.At this point, He introduces herself as his native English speaker and goes on to say that if you contine to work on social scnce\n",
      "\n",
      "Generated text by FP32 model: Most of the course is about the semantic content of language but there are also interesting topics to be learned from the service features except statistics in characters in documents. At this point, she introduces herself as a native English speaker and goes on to say that if you continue to work on social science, you will continue to be successful.\n",
      "\n",
      "Generated text by INT8 model: Most of the course is about the semantic content of language but there are also interesting topics to be learned from the service features except statistics in characters in documents. At this point, she introduces himself as a native English speaker and goes on to say that if you continue to work on social science, you will continue to do so.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input text:                   {default_text}\\n\")\n",
    "print(f'Generated text by FP32 model: {corrected_text_fp32}\\n')\n",
    "print(f'Generated text by INT8 model: {corrected_text_int8}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84eb3d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Compare model size, performance and accuracy [$\\Uparrow$](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb83a1-e071-4db1-8bdf-f8d58589871f",
   "metadata": {},
   "source": [
    "First, we compare file size of `FP32` and `INT8` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5473e21d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:27:08.903486600Z",
     "start_time": "2023-09-18T19:27:08.897908Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: openvino_decoder_with_past_model\n",
      "    * FP32 IR model size: 1658150.16 KB\n",
      "    * INT8 IR model size: 513467.73 KB\n",
      "    * Model compression rate: 3.229\n"
     ]
    }
   ],
   "source": [
    "def calculate_compression_rate(model_path_ov, model_path_ov_int8):\n",
    "    model_size_fp32 = model_path_ov.with_suffix(\".bin\").stat().st_size / 1024\n",
    "    model_size_int8 = model_path_ov_int8.with_suffix(\".bin\").stat().st_size / 1024\n",
    "    print(f\"Model: {model_path_ov.stem}\")\n",
    "    print(f\"    * FP32 IR model size: {model_size_fp32:.2f} KB\")\n",
    "    print(f\"    * INT8 IR model size: {model_size_int8:.2f} KB\")\n",
    "    print(f\"    * Model compression rate: {model_size_fp32 / model_size_int8:.3f}\")\n",
    "\n",
    "calculate_compression_rate(grammar_corrector_dir / \"openvino_decoder_with_past_model.xml\", quantized_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f5002-430d-4951-af87-7226392522b0",
   "metadata": {},
   "source": [
    "Second, we compare two grammar correction pipelines from performance and accuracy stand points.\n",
    "\n",
    "We again use [jfleg](https://huggingface.co/datasets/jfleg) dataset, but in this case the test split it selected. One dataset sample consists of a text with errors as input and several corrected versions as labels.\n",
    "\n",
    "When measuring accuracy we use mean `(1 - WER)` against corrected text versions, where WER is Word Error Rate metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a7cd65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:28:44.024236200Z",
     "start_time": "2023-09-18T19:27:08.902338800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8337d1783f234293afa163817c25e055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20786/4225757429.py:24: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  return original_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results of FP32 grammar correction pipeline. Accuracy: 67.59%. Time: 53.54 sec.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42c3132709a4ef78df08a896194a140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results of INT8 grammar correction pipeline. Accuracy: 68.80%. Time: 39.50 sec.\n",
      "Performance speedup: 1.356\n",
      "Accuracy drop :-1.21%.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from jiwer import wer, wer_standardize\n",
    "\n",
    "TEST_DATASET_SIZE = 50\n",
    "test_dataset = datasets.load_dataset(\"jfleg\", split=f\"test[:{TEST_DATASET_SIZE}]\")\n",
    "\n",
    "def calculate_inference_time_and_accuracy(grammar_corrector_pipe):\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "    inference_time = []\n",
    "    for data_item in tqdm(test_dataset, total=TEST_DATASET_SIZE, desc=\"Evaluation\"):\n",
    "        input_text = data_item[\"sentence\"]  # e.g., \"For not use car . \"\n",
    "        references = data_item[\"corrections\"]  # e.g., [ \"Not for use with a car . \", \"Do not use in the car . \", \"Car not for use . \", \"Can not use the car . \" ]\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        corrected_text = correct_text(input_text, grammar_checker_pipe, grammar_corrector_pipe, disable_tqdm=True)\n",
    "        end_time = time.perf_counter()\n",
    "        delta_time = end_time - start_time\n",
    "\n",
    "        ground_truths.extend(references)\n",
    "        predictions.extend([corrected_text] * len(references))\n",
    "        inference_time.append(delta_time)\n",
    "\n",
    "    word_accuracy = (1 - wer(ground_truths, predictions, reference_transform=wer_standardize, hypothesis_transform=wer_standardize)) * 100\n",
    "    sum_inference_time =sum(inference_time)\n",
    "    return sum_inference_time, word_accuracy\n",
    "\n",
    "inference_time_fp32, accuracy_fp32 = calculate_inference_time_and_accuracy(grammar_corrector_pipe_fp32)\n",
    "print(f\"Evaluation results of FP32 grammar correction pipeline. Accuracy: {accuracy_fp32:.2f}%. Time: {inference_time_fp32:.2f} sec.\")\n",
    "inference_time_int8, accuracy_int8 = calculate_inference_time_and_accuracy(grammar_corrector_pipe_int8)\n",
    "print(f\"Evaluation results of INT8 grammar correction pipeline. Accuracy: {accuracy_int8:.2f}%. Time: {inference_time_int8:.2f} sec.\")\n",
    "print(f\"Performance speedup: {inference_time_fp32 / inference_time_int8:.3f}\")\n",
    "print(f\"Accuracy drop :{accuracy_fp32 - accuracy_int8:.2f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bebbeb-176a-43f0-99d1-e96e6db60ccf",
   "metadata": {},
   "source": [
    "## Interactive demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87cfc164-46f4-4310-b088-7504f3f42da1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T19:28:44.027129200Z",
     "start_time": "2023-09-18T19:28:44.027129200Z"
    },
    "test_replace": {
     "    demo.queue().launch(debug=True)": "    demo.queue().launch()",
     "    demo.queue().launch(share=True, debug=True)": "    demo.queue().launch(share=True)"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def correct(text, _=gr.Progress(track_tqdm=True)):\n",
    "    return correct_text(text, grammar_checker_pipe, grammar_corrector_pipe_int8)\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    correct,\n",
    "    gr.Textbox(label=\"Text\"),\n",
    "    gr.Textbox(label=\"Correction\"),\n",
    "    examples=[default_text],\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "try:\n",
    "    demo.queue().launch(debug=True, server_port=7860)\n",
    "except Exception:\n",
    "    demo.queue().launch(share=True, debug=True)\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "# demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# Read more in the docs: https://gradio.app/docs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04987ecd2c7a48c48a599fd1f23ea586": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2cf6aa309bfe43ae9f694ee2e26b27eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8865247c89cc4a60b22380b09547f508",
       "style": "IPY_MODEL_7bd41bb8c1ad4bf6a2ee29a96900eea6",
       "value": "correcting text..: 100%"
      }
     },
     "40d0eca7cba048a79102b84d1e14a802": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "44b1c27db36641e58268ac097c32240c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextareaModel",
      "state": {
       "description": "your text",
       "layout": "IPY_MODEL_a2d5a746e06e4e81aa4d2c4faadd9ed7",
       "style": "IPY_MODEL_04987ecd2c7a48c48a599fd1f23ea586",
       "value": "Most of the course is about semantic or  content of language but there are also interesting topics to be learned from the servicefeatures except statistics in characters in documents.At this point, He introduces herself as his native English speaker and goes on to say that if you contine to work on social scnce"
      }
     },
     "62b441aad45e407c9b108778bb7819b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_40d0eca7cba048a79102b84d1e14a802",
       "max": 1,
       "style": "IPY_MODEL_63a5bb6794e14161b6a507fa233c7ff6",
       "value": 1
      }
     },
     "63a5bb6794e14161b6a507fa233c7ff6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6f75e3b03db54b3aaf74c8039a46c904": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_fd18874977234332aba5ea97af20e016",
       "style": "IPY_MODEL_ebec355dbe87452fbceb2bde75fa716c",
       "value": " 1/1 [00:04&lt;00:00,  4.42s/it]"
      }
     },
     "7bd41bb8c1ad4bf6a2ee29a96900eea6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "873341cd0b374e6abd68ead9e0fb5eea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2cf6aa309bfe43ae9f694ee2e26b27eb",
        "IPY_MODEL_62b441aad45e407c9b108778bb7819b7",
        "IPY_MODEL_6f75e3b03db54b3aaf74c8039a46c904"
       ],
       "layout": "IPY_MODEL_964b79b8f6c84659861916b5098d44b4"
      }
     },
     "8865247c89cc4a60b22380b09547f508": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "964b79b8f6c84659861916b5098d44b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9e896b2be776416b906d41bca2cc56a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a2d5a746e06e4e81aa4d2c4faadd9ed7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "a6cc5a5b6a764c07883ab0d2aa6b7ae9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "CPU",
        "GPU",
        "AUTO"
       ],
       "description": "Device:",
       "index": 2,
       "layout": "IPY_MODEL_ca698f18cc0c48abac11c1d6d4075e91",
       "style": "IPY_MODEL_9e896b2be776416b906d41bca2cc56a6"
      }
     },
     "ca698f18cc0c48abac11c1d6d4075e91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ebec355dbe87452fbceb2bde75fa716c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fd18874977234332aba5ea97af20e016": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
